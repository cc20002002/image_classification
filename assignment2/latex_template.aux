\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{liu2016classification}
\citation{hardle2007applied}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{Bishop:2006:PRM:1162264}
\citation{pmlr-v20-biggio11}
\citation{liu2016classification}
\citation{brodley1996identifying}
\citation{NIPS2012_4500,Seeger:2003:PGE:944919.944929,Cortes1995}
\citation{Fernandez-Delgado:2014:WNH:2627435.2697065}
\citation{frenay2014classification}
\citation{liu2016classification}
\citation{frenay2014classification}
\citation{frenay2014classification}
\citation{yang2018adasampling}
\citation{pmlr-v20-biggio11}
\citation{liu2016classification}
\citation{liu2016classification}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}}
\citation{Boser:1992:TAO:130385.130401}
\citation{Cortes1995}
\citation{Fernandez-Delgado:2014:WNH:2627435.2697065}
\citation{hastie01statisticallearning}
\citation{hastie01statisticallearning}
\citation{NIPS2012_4500}
\citation{Cortes1995,Seeger:2003:PGE:944919.944929}
\citation{frenay2014classification}
\citation{jonsson2002support}
\citation{jonsson2002support}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{4}{section.3}}
\newlabel{method}{{3}{4}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Base model: support vector machine}{4}{subsection.3.1}}
\newlabel{sec:svm}{{3.1}{4}{Base model: support vector machine}{subsection.3.1}{}}
\newlabel{eq:decision}{{1}{4}{Base model: support vector machine}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Support vector machine resists over-fitting}{4}{subsubsection.3.1.1}}
\newlabel{sec:over}{{3.1.1}{4}{Support vector machine resists over-fitting}{subsubsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Support vector machine is not robust to label noise}{4}{subsubsection.3.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Preprocess}{4}{subsection.3.2}}
\newlabel{preproc}{{3.2}{4}{Preprocess}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Photometric normalisation improves classification performance}{4}{subsubsection.3.2.1}}
\citation{liu2016classification}
\citation{DBLP:journals/jmlr/KanamoriHS09}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Principal component analysis reduces dimensionality}{5}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The original dataset is balanced}{5}{subsection.3.3}}
\newlabel{sec:1}{{3.3}{5}{The original dataset is balanced}{subsection.3.3}{}}
\newlabel{eq:ps}{{2}{5}{The original dataset is balanced}{equation.3.2}{}}
\newlabel{eq:e1}{{3}{5}{The original dataset is balanced}{equation.3.3}{}}
\newlabel{eq:em1}{{4}{5}{The original dataset is balanced}{equation.3.4}{}}
\newlabel{eq:exp}{{5}{5}{The original dataset is balanced}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Flip rates estimation}{5}{subsection.3.4}}
\newlabel{method2}{{3.4}{5}{Flip rates estimation}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Density ratio method estimates conditional probability}{5}{subsubsection.3.4.1}}
\newlabel{method22}{{3.4.1}{5}{Density ratio method estimates conditional probability}{subsubsection.3.4.1}{}}
\newlabel{eq:baye}{{6}{5}{Density ratio method estimates conditional probability}{equation.3.6}{}}
\newlabel{eq:gauss}{{7}{5}{Density ratio method estimates conditional probability}{equation.3.7}{}}
\citation{liu2016classification}
\citation{pmlr-v20-biggio11}
\citation{pmlr-v20-biggio11}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Minimum of conditional probability estimates flip rates}{6}{subsubsection.3.4.2}}
\newlabel{method23}{{3.4.2}{6}{Minimum of conditional probability estimates flip rates}{subsubsection.3.4.2}{}}
\newlabel{eq:fliprate}{{3.4.2}{6}{Minimum of conditional probability estimates flip rates}{subsubsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Method 1: Expectation Maximisation}{6}{subsection.3.5}}
\newlabel{1st}{{3.5}{6}{Method 1: Expectation Maximisation}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Expectation Maximisation derives loss function}{6}{subsubsection.3.5.1}}
\newlabel{eq:dual}{{8}{6}{Expectation Maximisation derives loss function}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Bernoulli random variable models latent label noise}{6}{subsubsection.3.5.2}}
\citation{pmlr-v20-biggio11}
\citation{Theodoridis:2008:PRF:1457541}
\newlabel{eq:dual2}{{9}{7}{Bernoulli random variable models latent label noise}{equation.3.9}{}}
\newlabel{eq:dual3}{{10}{7}{Bernoulli random variable models latent label noise}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Modifying kernel improves robustness against label noise}{7}{subsubsection.3.5.3}}
\newlabel{sec:mod}{{3.5.3}{7}{Modifying kernel improves robustness against label noise}{subsubsection.3.5.3}{}}
\newlabel{matn1}{{1}{7}{Customised kernel for Expectation Maximisation algorithm}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Customised kernel for Expectation Maximisation algorithm}{7}{lstlisting.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Proof: proposed kernel matrix is positive definite}{7}{subsubsection.3.5.4}}
\newlabel{sec:prove}{{3.5.4}{7}{Proof: proposed kernel matrix is positive definite}{subsubsection.3.5.4}{}}
\citation{Schur1911}
\citation{Theodoridis:2008:PRF:1457541}
\citation{liu2016classification}
\citation{liu2016classification}
\citation{liu2016classification}
\citation{Sugiyama10densityratio}
\citation{liu2016classification}
\citation{Platt99probabilisticoutputs}
\citation{liu2016classification}
\citation{liu2016classification}
\citation{liu2016classification}
\newlabel{eq:p0}{{11}{8}{Proof: proposed kernel matrix is positive definite}{equation.3.11}{}}
\newlabel{eq:p1}{{12}{8}{Proof: proposed kernel matrix is positive definite}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Method 2: Importance Reweighting}{8}{subsection.3.6}}
\newlabel{3rd}{{3.6}{8}{Method 2: Importance Reweighting}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1}Sigmoid function estimates conditional probability}{8}{subsubsection.3.6.1}}
\newlabel{sigmoid}{{3.6.1}{8}{Sigmoid function estimates conditional probability}{subsubsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2}Reweighting coefficient improves robustness against label noise}{8}{subsubsection.3.6.2}}
\newlabel{beta}{{13}{8}{Reweighting coefficient improves robustness against label noise}{equation.3.13}{}}
\citation{Wu03probabilityestimates}
\citation{hastie01statisticallearning}
\newlabel{eq:weighting}{{15}{9}{Reweighting coefficient improves robustness against label noise}{equation.3.15}{}}
\newlabel{eq:loss}{{16}{9}{Reweighting coefficient improves robustness against label noise}{equation.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Method 3: heuristic approach by relabelling}{9}{subsection.3.7}}
\newlabel{2nd}{{3.7}{9}{Method 3: heuristic approach by relabelling}{subsection.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.1}Conditional probability filters samples}{9}{subsubsection.3.7.1}}
\newlabel{2nd2}{{3.7.1}{9}{Conditional probability filters samples}{subsubsection.3.7.1}{}}
\newlabel{eq:filter}{{17}{9}{Conditional probability filters samples}{equation.3.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2}Pre-training model corrects labels}{9}{subsubsection.3.7.2}}
\citation{NIPS2012_4500,Cortes1995,Seeger:2003:PGE:944919.944929}
\citation{Walck:1996cca}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Tuning hyperparameters}{10}{subsection.3.8}}
\newlabel{tune}{{3.8}{10}{Tuning hyperparameters}{subsection.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Bootstrap constructs confidence intervals and hypothesis tests}{10}{subsection.3.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.1}Bootstrapping percentile confidence intervals}{10}{subsubsection.3.9.1}}
\newlabel{ci}{{3.9.1}{10}{Bootstrapping percentile confidence intervals}{subsubsection.3.9.1}{}}
\newlabel{eq:boot}{{18}{10}{Bootstrapping percentile confidence intervals}{equation.3.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.2}Kolmogorov-Smirnov test compares the accuracy of algorithms}{10}{subsubsection.3.9.2}}
\newlabel{sec:ks}{{3.9.2}{10}{Kolmogorov-Smirnov test compares the accuracy of algorithms}{subsubsection.3.9.2}{}}
\newlabel{epdf}{{19}{10}{Kolmogorov-Smirnov test compares the accuracy of algorithms}{equation.3.19}{}}
\newlabel{teststatistic}{{20}{10}{Kolmogorov-Smirnov test compares the accuracy of algorithms}{equation.3.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces HPC}}{11}{figure.1}}
\newlabel{fig:HPC}{{1}{11}{HPC}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{11}{section.4}}
\newlabel{result}{{4}{11}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment Setting}{11}{subsection.4.1}}
\newlabel{multi}{{2}{11}{Multi-threading using the \texttt {multiprocessing} package of \texttt {Python}}{lstlisting.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Multi-threading using the \texttt  {multiprocessing} package of \texttt  {Python}.}{11}{lstlisting.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Preprocessing}{11}{subsection.4.2}}
\citation{liu2016classification}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison between original images and processed images. The first row shows the original images. The second row shows the images after applying photometric normalisation. The third row shows the \textsc  {pca}-processed images.}}{12}{figure.2}}
\newlabel{fig:Compare_Image}{{2}{12}{Comparison between original images and processed images. The first row shows the original images. The second row shows the images after applying photometric normalisation. The third row shows the \textsc {pca}-processed images}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Photometric normalisation improves convergence}{12}{subsubsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Principal component analysis speeds up algorithms}{12}{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Discuss flip rate}{12}{subsection.4.3}}
\citation{Roberts:2014:MED:2746455}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Mean, standard deviation and confidence interval of accuracy and average running time for combinations of algorithm and datasets. In the algorithm column, \textsc  {em} represents the Expectation Maximisation, \textsc  {ir} represents Importance Reweighting, and \textsc  {r} represents Relabelling. We also include an ordinary \textsc  {svm}\ as a benchmark.}}{13}{table.1}}
\newlabel{tab:Meansd}{{1}{13}{Mean, standard deviation and confidence interval of accuracy and average running time for combinations of algorithm and datasets. In the algorithm column, \textsc {em} represents the Expectation Maximisation, \textsc {ir} represents Importance Reweighting, and \textsc {r} represents Relabelling. We also include an ordinary \svm \ as a benchmark}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Selected hyperparameters for classification models against \textsc  {ccn}. The hyperparameters with subscript\nobreakspace  {}$1$, that is, the bandwidth\nobreakspace  {}$\gamma _1$ and soft margin regularisation\nobreakspace  {}$\frac  {1}{2n\lambda _1}$, are for the pre-training models. Those with subscript\nobreakspace  {}$2$ are for the classification model. Note that Expectation Maximisation does not have a pre-training model. In the algorithm column, \textsc  {em} represents the algorithm of Expectation Maximisation, \textsc  {ir} represents the algorithm Importance Reweighting and \textsc  {r} represents the algorithm of Relabelling.}}{13}{table.2}}
\newlabel{tab:hyperparameter}{{2}{13}{Selected hyperparameters for classification models against \textsc {ccn}. The hyperparameters with subscript~$1$, that is, the bandwidth~$\gamma _1$ and soft margin regularisation~$\frac {1}{2n\lambda _1}$, are for the pre-training models. Those with subscript~$2$ are for the classification model. Note that Expectation Maximisation does not have a pre-training model. In the algorithm column, \textsc {em} represents the algorithm of Expectation Maximisation, \textsc {ir} represents the algorithm Importance Reweighting and \textsc {r} represents the algorithm of Relabelling}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Selected hyperparameters}{13}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Hyperparameters for algorithms}{13}{subsubsection.4.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Hyperparameters for algorithms}{13}{subsubsection.4.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Regression estimates running time}{13}{subsection.4.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Number of samples versus average running time on a log-log scale. The slope of the straight lines on this scale approximates the computational complexity in sample size\nobreakspace  {}$n$. The ratio for a pair of points from the same algorithm and sample size approximates complexity in number of features\nobreakspace  {}$p$. }}{14}{figure.3}}
\newlabel{fig:speed}{{3}{14}{Number of samples versus average running time on a log-log scale. The slope of the straight lines on this scale approximates the computational complexity in sample size~$n$. The ratio for a pair of points from the same algorithm and sample size approximates complexity in number of features~$p$}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Compare algorithms and datasets}{14}{subsection.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Hypothesis tests justify visualisations}{14}{subsubsection.4.6.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The result of the hypothesis test with the training sample size of $8,000$. The result is `reject' indicates we have statistical evidence at $95\%$ confidence level to conclude one algorithm is more accurate than the other.}}{15}{table.3}}
\newlabel{tab:HypothesisTest}{{3}{15}{The result of the hypothesis test with the training sample size of $8,000$. The result is `reject' indicates we have statistical evidence at $95\%$ confidence level to conclude one algorithm is more accurate than the other}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The density function of accuracy estimated by kernel smoothing. The three colours correspond to three algorithms. The left density plots are estimated using the fashion-\textsc  {mnist}\ dataset. The middle and right density plots are estimated from \textsc  {cifar} dataset, without and with \textsc  {pca}.}}{15}{figure.4}}
\newlabel{fig:Density}{{4}{15}{The density function of accuracy estimated by kernel smoothing. The three colours correspond to three algorithms. The left density plots are estimated using the \mnist \ dataset. The middle and right density plots are estimated from \textsc {cifar} dataset, without and with \textsc {pca}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Expectation Maximisation is the fastest}{15}{subsubsection.4.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Box plots of the accuracy results against running time. Different colours of borders represent different learning algorithms. Different colours filling the boxes represent the two original datasets and \textsc  {pca} preprocessed \textsc  {cifar}. The vertical edges of boxes represent the $1$st and $3$rd quantiles of accuracy for each setting, and the end of vertical lines represent $1.5$ inter quantile range from these two quantiles. The dots are outliers. The right two boxes and the third bottom middle pink box are the accuracies of classifying original \textsc  {cifar} dataset. The bottom left three boxes are the accuracies of classifying the \textsc  {cifar} dataset with \textsc  {pca}. The top left three boxes are the accuracies of classifying fashion-\textsc  {mnist}\ dataset.}}{16}{figure.5}}
\newlabel{fig:Boxplot}{{5}{16}{Box plots of the accuracy results against running time. Different colours of borders represent different learning algorithms. Different colours filling the boxes represent the two original datasets and \textsc {pca} preprocessed \textsc {cifar}. The vertical edges of boxes represent the $1$st and $3$rd quantiles of accuracy for each setting, and the end of vertical lines represent $1.5$ inter quantile range from these two quantiles. The dots are outliers. The right two boxes and the third bottom middle pink box are the accuracies of classifying original \textsc {cifar} dataset. The bottom left three boxes are the accuracies of classifying the \textsc {cifar} dataset with \textsc {pca}. The top left three boxes are the accuracies of classifying \mnist \ dataset}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3}\textsc  {cifar} is more difficult to classify}{16}{subsubsection.4.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.4}Relabelling approach is inconsistent and less robust}{16}{subsubsection.4.6.4}}
\citation{liu2016classification}
\citation{liu2016classification}
\citation{NIPS1999_1672}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The accuracy of algorithms versus sample size for all three methods on both datasets. The sample size increases from $2,000$ to $10,000$. The coloured regions near the lines indicate a $95\%$ confidence interval. Three colours of the lines represent different algorithms. The three node shapes and the colours of the confidence intervals are corresponding to different datasets. }}{17}{figure.6}}
\newlabel{fig:acc}{{6}{17}{The accuracy of algorithms versus sample size for all three methods on both datasets. The sample size increases from $2,000$ to $10,000$. The coloured regions near the lines indicate a $95\%$ confidence interval. Three colours of the lines represent different algorithms. The three node shapes and the colours of the confidence intervals are corresponding to different datasets}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Discussion and personal reflection}{17}{subsection.4.7}}
\citation{liu2016classification}
\citation{liu2016classification}
\citation{liu2016classification}
\bibstyle{unsrtnat}
\bibdata{reference}
\bibcite{liu2016classification}{{1}{2016}{{Liu and Tao}}{{}}}
\bibcite{hardle2007applied}{{2}{2007}{{H{\"a}rdle and Simar}}{{}}}
\bibcite{Bishop:2006:PRM:1162264}{{3}{2006}{{Bishop}}{{}}}
\bibcite{pmlr-v20-biggio11}{{4}{2011}{{Biggio et~al.}}{{Biggio, Nelson, and Laskov}}}
\bibcite{brodley1996identifying}{{5}{1996}{{Brodley et~al.}}{{Brodley, Friedl, et~al.}}}
\bibcite{NIPS2012_4500}{{6}{2012}{{Jin and Wang}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{18}{section.5}}
\newlabel{headings}{{5}{18}{Conclusion}{section.5}{}}
\bibcite{Seeger:2003:PGE:944919.944929}{{7}{2003}{{Seeger}}{{}}}
\bibcite{Cortes1995}{{8}{1995}{{Cortes and Vapnik}}{{}}}
\bibcite{Fernandez-Delgado:2014:WNH:2627435.2697065}{{9}{2014}{{Fern\'{a}ndez-Delgado et~al.}}{{Fern\'{a}ndez-Delgado, Cernadas, Barro, and Amorim}}}
\bibcite{frenay2014classification}{{10}{2014}{{Fr{\'e}nay and Verleysen}}{{}}}
\bibcite{yang2018adasampling}{{11}{2018}{{Yang et~al.}}{{Yang, Ormerod, Liu, Ma, Zomaya, and Yang}}}
\bibcite{Boser:1992:TAO:130385.130401}{{12}{1992}{{Boser et~al.}}{{Boser, Guyon, and Vapnik}}}
\bibcite{hastie01statisticallearning}{{13}{2001}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{jonsson2002support}{{14}{2002}{{Jonsson et~al.}}{{Jonsson, Kittler, Li, and Matas}}}
\bibcite{DBLP:journals/jmlr/KanamoriHS09}{{15}{2009}{{Kanamori et~al.}}{{Kanamori, Hido, and Sugiyama}}}
\bibcite{Theodoridis:2008:PRF:1457541}{{16}{2008}{{Theodoridis and Koutroumbas}}{{}}}
\bibcite{Schur1911}{{17}{1911}{{Schur}}{{}}}
\bibcite{Sugiyama10densityratio}{{18}{2010}{{Sugiyama and et~al.}}{{}}}
\bibcite{Platt99probabilisticoutputs}{{19}{1999}{{Platt}}{{}}}
\bibcite{Wu03probabilityestimates}{{20}{2003}{{Wu et~al.}}{{Wu, Lin, and Weng}}}
\bibcite{Walck:1996cca}{{21}{1996}{{Walck}}{{}}}
\bibcite{Roberts:2014:MED:2746455}{{22}{2014}{{Roberts}}{{}}}
\bibcite{NIPS1999_1672}{{23}{2000}{{Ghahramani and Beal}}{{}}}
\ulp@afterend
