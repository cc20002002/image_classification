\BOOKMARK [1][]{section.1}{Introduction}{}% 1
\BOOKMARK [1][]{section.2}{Related work}{}% 2
\BOOKMARK [1][]{section.3}{Methods}{}% 3
\BOOKMARK [2][]{subsection.3.1}{Base model: support vector machine}{section.3}% 4
\BOOKMARK [3][]{subsubsection.3.1.1}{Support vector machine resists over-fitting}{subsection.3.1}% 5
\BOOKMARK [3][]{subsubsection.3.1.2}{Support vector machine is not robust to label noise}{subsection.3.1}% 6
\BOOKMARK [2][]{subsection.3.2}{Preprocess}{section.3}% 7
\BOOKMARK [3][]{subsubsection.3.2.1}{Photometric normalisation improves classification performance}{subsection.3.2}% 8
\BOOKMARK [3][]{subsubsection.3.2.2}{Principal component analysis reduces dimensionality}{subsection.3.2}% 9
\BOOKMARK [2][]{subsection.3.3}{The original dataset is balanced}{section.3}% 10
\BOOKMARK [2][]{subsection.3.4}{Flip rates estimation}{section.3}% 11
\BOOKMARK [3][]{subsubsection.3.4.1}{Density ratio method estimates conditional probability}{subsection.3.4}% 12
\BOOKMARK [3][]{subsubsection.3.4.2}{Minimum of conditional probability estimates flip rates}{subsection.3.4}% 13
\BOOKMARK [2][]{subsection.3.5}{Method 1: Expectation Maximisation}{section.3}% 14
\BOOKMARK [3][]{subsubsection.3.5.1}{Expectation Maximisation derives loss function}{subsection.3.5}% 15
\BOOKMARK [3][]{subsubsection.3.5.2}{Bernoulli random variable models latent label noise}{subsection.3.5}% 16
\BOOKMARK [3][]{subsubsection.3.5.3}{Modifying kernel improves robustness against label noise}{subsection.3.5}% 17
\BOOKMARK [3][]{subsubsection.3.5.4}{Proof: proposed kernel matrix is positive definite}{subsection.3.5}% 18
\BOOKMARK [2][]{subsection.3.6}{Method 2: Importance Reweighting}{section.3}% 19
\BOOKMARK [3][]{subsubsection.3.6.1}{Sigmoid function estimates conditional probability}{subsection.3.6}% 20
\BOOKMARK [3][]{subsubsection.3.6.2}{Reweighting coefficient improves robustness against label noise}{subsection.3.6}% 21
\BOOKMARK [2][]{subsection.3.7}{Method 3: heuristic approach by relabelling}{section.3}% 22
\BOOKMARK [3][]{subsubsection.3.7.1}{Conditional probability filters samples}{subsection.3.7}% 23
\BOOKMARK [3][]{subsubsection.3.7.2}{Pre-training model corrects labels}{subsection.3.7}% 24
\BOOKMARK [2][]{subsection.3.8}{Tuning hyperparameters}{section.3}% 25
\BOOKMARK [2][]{subsection.3.9}{Bootstrap constructs confidence intervals and hypothesis tests}{section.3}% 26
\BOOKMARK [3][]{subsubsection.3.9.1}{Bootstrapping percentile confidence intervals}{subsection.3.9}% 27
\BOOKMARK [3][]{subsubsection.3.9.2}{Kolmogorov-Smirnov test compares the accuracy of algorithms}{subsection.3.9}% 28
\BOOKMARK [1][]{section.4}{Experiments}{}% 29
\BOOKMARK [2][]{subsection.4.1}{Experiment Setting}{section.4}% 30
\BOOKMARK [2][]{subsection.4.2}{Preprocessing}{section.4}% 31
\BOOKMARK [3][]{subsubsection.4.2.1}{Photometric normalisation improves convergence}{subsection.4.2}% 32
\BOOKMARK [3][]{subsubsection.4.2.2}{Principal component analysis speeds up algorithms}{subsection.4.2}% 33
\BOOKMARK [2][]{subsection.4.3}{Discuss flip rate}{section.4}% 34
\BOOKMARK [2][]{subsection.4.4}{Selected hyperparameters}{section.4}% 35
\BOOKMARK [3][]{subsubsection.4.4.1}{Hyperparameters for algorithms}{subsection.4.4}% 36
\BOOKMARK [3][]{subsubsection.4.4.2}{Hyperparameters for algorithms}{subsection.4.4}% 37
\BOOKMARK [2][]{subsection.4.5}{Regression estimates running time}{section.4}% 38
\BOOKMARK [2][]{subsection.4.6}{Compare algorithms and datasets}{section.4}% 39
\BOOKMARK [3][]{subsubsection.4.6.1}{Hypothesis tests justify visualisations}{subsection.4.6}% 40
\BOOKMARK [3][]{subsubsection.4.6.2}{Expectation Maximisation is the fastest}{subsection.4.6}% 41
\BOOKMARK [3][]{subsubsection.4.6.3}{cifar is more difficult to classify}{subsection.4.6}% 42
\BOOKMARK [3][]{subsubsection.4.6.4}{Relabelling approach is inconsistent and less robust}{subsection.4.6}% 43
\BOOKMARK [2][]{subsection.4.7}{Discussion and personal reflection}{section.4}% 44
\BOOKMARK [1][]{section.5}{Conclusion}{}% 45
