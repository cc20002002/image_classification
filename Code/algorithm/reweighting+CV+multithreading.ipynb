{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca explained variance: 0.8560751474986309\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sat Oct 13 00:24:24 2018\n",
    "\n",
    "@author: chenc\n",
    "reweighting method from tut 2\n",
    "dataset1 improves the accuracy  to 0.946\n",
    "dataset1 improves the accuracy  to 0.855\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.datasets import load_iris\n",
    "#from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from os import cpu_count\n",
    "from sklearn.decomposition import IncrementalPCA as PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "#from random import sample\n",
    "\n",
    "\n",
    "dset=2\n",
    "plot=0\n",
    "\n",
    "\n",
    "dataset1 = np.load('../input_data/mnist_dataset.npz')\n",
    "size_image1 = 28\n",
    "dim_image1=1 \n",
    "dataset2 = np.load('../input_data/cifar_dataset.npz')\n",
    "size_image2 =32\n",
    "dim_image2 =3\n",
    "#size_image=28\n",
    "#dim_image=1\n",
    "\n",
    "#to store the data splits\n",
    "data_cache={}\n",
    "\n",
    "#transform dataset1\n",
    "Xtr1 = dataset1 ['Xtr'].astype(float)\n",
    "Str1 = dataset1 ['Str'].ravel()\n",
    "Xts1 = dataset1 ['Xts'].astype(float)\n",
    "Yts1 = dataset1 ['Yts'].ravel()\n",
    "scaler = StandardScaler()\n",
    "Xts1 = scaler.fit_transform(Xts1.T).T\n",
    "Xtr1 = scaler.fit_transform(Xtr1.T).T\n",
    "data_cache[1]=(Xtr1,Str1,Xts1,Yts1)\n",
    "\n",
    "#transform dataset2\n",
    "Xtr2 = dataset2 ['Xtr'].astype(float)\n",
    "Str2 = dataset2 ['Str'].ravel()\n",
    "Xts2 = dataset2 ['Xts'].astype(float)\n",
    "Yts2 = dataset2 ['Yts'].ravel()\n",
    "#scaler = StandardScaler()\n",
    "Xts2 = scaler.fit_transform(Xts2.T).T\n",
    "Xtr2 = scaler.fit_transform(Xtr2.T).T\n",
    "data_cache[2]=(Xtr2,Str2,Xts2,Yts2)\n",
    "\n",
    "#Xtr=Xtr.reshape(10000,dim_image,size_image,size_image).transpose([0,2, 3, 1]).mean(3).reshape(10000,size_image*size_image)\n",
    "#Xts=Xts.reshape(2000,dim_image,size_image,size_image).transpose([0,2, 3, 1]).mean(3).reshape(2000,size_image*size_image)\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(Xtr2)\n",
    "Xtr2=pca.transform(Xtr2)\n",
    "Xts2=pca.transform(Xts2)\n",
    "print('pca explained variance:',sum(pca.explained_variance_ratio_))\n",
    "if plot:\n",
    "    xplot=scaler.fit_transform(pca.inverse_transform(Xts2).T).T\n",
    "##1plt.jet()\n",
    "\n",
    "if plot:\n",
    "    plt.figure()\n",
    "    for i in range(0,30):\n",
    "        if dset==1:\n",
    "            image=xplot[i,].reshape(dim_image1,size_image1,size_image1).transpose([1, 2, 0])\n",
    "            plt.subplot(5, 6, i+1)\n",
    "            plt.imshow(image[:,:,:],interpolation='bicubic')\n",
    "            plt.title(Yts1[i])\n",
    "        else:\n",
    "            image=xplot[i,].reshape(dim_image2,size_image2,size_image2).transpose([1, 2, 0])\n",
    "            plt.subplot(5, 6, i+1)\n",
    "            plt.imshow(image[:,:,:],interpolation='bicubic')\n",
    "            plt.title(Yts2[i])\n",
    "\n",
    "#indices = np.random.choice(Xts.shape[0],int(Xts.shape[0]*0.8), replace=False)\n",
    "\n",
    "def estimateBeta(S,prob,rho0,rho1):\n",
    "    S=S.astype(int)\n",
    "    rho=np.array([rho1,rho0])\n",
    "    #rho=np.tile(np.array([rho1, rho0]).reshape(-1,1),700).T\n",
    "    #print(rho[S])\n",
    "    \n",
    "    #print(S)\n",
    "    prob=prob[:,0]*(1-S[:])+prob[:,1]*(S[:])\n",
    "    #print(sum(prob>.5)/700)\n",
    "    #print(S[0:11])\n",
    "    beta=(prob[:]-rho[S].ravel())/(1-rho0-rho1)/prob[:]\n",
    "    return beta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# dset chooses dataset. num_run determines the number of iterations.\n",
    "def cv_reweighting(dset):\n",
    "    print('start of reweighting algorithm')\n",
    "    Xtr,Str,Xts,Yts = data_cache[dset]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Xtr, Str, test_size=0.2)\n",
    "    #clf1 is the first classifier while clf2 is the second\n",
    "    if dset==1:\n",
    "        clf1 = svm.SVC(C=.8,gamma=0.000225,probability=True)\n",
    "    else:\n",
    "        #removed 'gamma=scale'. should be the default.\n",
    "        clf1 = svm.SVC(probability=True,C=.4)\n",
    "        \n",
    "    clf1.fit(X_train,y_train)\n",
    "    #print(clf.score(Xts,Yts))\n",
    "    #clf.score(Xtr,Str)\n",
    "    probS = clf1.predict_proba(X_train)\n",
    "    weights = estimateBeta(y_train, probS, 0.2, 0.4)\n",
    "    #print(weights.shape)\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        if weights[i] < 0:\n",
    "            weights[i] = 0.0    \n",
    "\n",
    "    if dset==2:\n",
    "        clf2 = svm.SVC(gamma=0.000225,C=0.8,probability=True)\n",
    "    else:\n",
    "        clf2 = svm.SVC(gamma=0.00865,C=.4,probability=True)\n",
    "\n",
    "    clf2.fit(X_train,y_train,sample_weight=weights)\n",
    "    #test_score=clf.score(Xts,Yts)\n",
    "    #clf.score(Xtr,Str)\n",
    "    # to accuracy 94.6 for dataset 1\n",
    "    # 85.5 for dataset 2.\n",
    "    return clf2.score(Xts,Yts)\n",
    "\n",
    "def run_algorithm(alg_type, dset, num_run):   #alg_type: type of the algorithm, choose from 'reweighting',...tbc\n",
    "    start=time.clock()\n",
    "    print('start of the whole algorithm with dataset',dset)\n",
    "    if alg_type=='reweighting':\n",
    "        pool = Pool(processes=cpu_count())\n",
    "        it = pool.map(cv_reweighting, range(num_run))  #using the number of runs\n",
    "        #test_score=np.zeros(num_run)\n",
    "        #for i in range(num_run):\n",
    "            #test_score[i]=cv_reweighting(1)\n",
    "    test_score= it\n",
    "    average_score=np.mean(test_score)\n",
    "    std_score=np.std(test_score)\n",
    "    print('average score: ',average_score,'\\nstandard deviation: ',std_score) # help to format here!\n",
    "\n",
    "    with open('result.txt', 'w') as f: #better way to output result? I would like they can be read into python easily\n",
    "        for item in test_score:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        end=time.clock()\n",
    "    print('total process time is',round(end-start,4),'sec')\n",
    "    \n",
    "    return average_score, std_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the whole algorithm with dataset 2\n",
      "start of reweighting algorithm\n"
     ]
    }
   ],
   "source": [
    "average_score1, std_score1 = run_algorithm('reweighting',1,10)\n",
    "average_score2, std_score2 = run_algorithm('reweighting',2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
