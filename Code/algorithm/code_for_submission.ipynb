{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5328 - Advanced Machine Learning\n",
    "\n",
    "## Assignment 2 - Learning with Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lecturer**: Tongliang Liu\n",
    "\n",
    "**Tutors** : Zhuozhuo Tu, Liu Liu\n",
    "\n",
    "**Group Members** : Chen Chen, Yutong Cao, Yixiong Fang\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "The goal of this assignment is to study how to learn with label noise. Specifically, you need to use at least two methods to classify real world images with noisy labels into a set of categories. Then, you need to compare the performance of these classifiers and analyze the robustness of label noise methods.\n",
    "The datasets are quite large, so you need to be smart on which methods you gonna use and perhaps perform a pre-processing step to reduce the amount of computation. Part of your marks will be a function of the performance of your classifier on the test set.\n",
    "\n",
    "**Reuirements:**\n",
    "- sklearn 0.20.0 (The develpmetn version, download here)\n",
    "- multiprocessing\n",
    "- numpy\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from os import cpu_count\n",
    "from sklearn.decomposition import IncrementalPCA as PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import exp\n",
    "from itertools import product\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "1. Training features and labels:\n",
    "    - Xtr: shape=(10000, d). There are 10, 000 instances. The raw data are 28 × 28 (for Fashion-MNIST) or 32 × 32 × 3 (for CIFAR) images, which are reshaped to features with dimension d = 784 or d = 3072.\n",
    "    - Str: shape=(10000, 1). There are 10, 000 noisy labels for the corre- sponding instances.\n",
    "    - These 10,000 instances belong to two categories. The corresponding labels for these two categories are 0 and 1. These training examples are with label noise. The flip rates are $ ρ_0 = p(S = 1|Y = 0) = 0.2 $ and $ ρ_1 =p(S=0|Y =1)=0.4 $, where $S$ and $Y$ are the variables of noisy labels and true labels, respectively.\n",
    "    - Note that do not use all the 10,000 examples to train your models. You are required to independently and randomly sample 8,000 examples from the 10,000 examples to train every classifier. The reported performance of each model should be the average performance of at least 10 learned classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loading Dataset: mnist====\n",
      "Traing Set:\n",
      "Xtr.shape = (10000, 784)\n",
      "Str.shape = (10000,)\n",
      "Testing Set:\n",
      "Xts.shape = (2000, 784)\n",
      "Yts.shape = (2000,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(filepath, name, standardlize=False, PCA=False, n=100):\n",
    "    dataset = np.load(filepath)\n",
    "    Xtr = dataset['Xtr'].astype(float)\n",
    "    Str = dataset['Str'].ravel()\n",
    "    Xts = dataset['Xts'].astype(float)\n",
    "    Yts = dataset['Yts'].ravel()\n",
    "    print('====Loading Dataset: {}===='.format(name))\n",
    "    print('Traing Set:')\n",
    "    print('Xtr.shape = {}'.format(Xtr.shape))\n",
    "    print('Str.shape = {}'.format(Str.shape))\n",
    "    print('Testing Set:')\n",
    "    print('Xts.shape = {}'.format(Xts.shape))\n",
    "    print('Yts.shape = {}'.format(Yts.shape))\n",
    "    if standardlize:\n",
    "        # TODO        \n",
    "        continue\n",
    "    if PCA:\n",
    "        # TODO        \n",
    "        continue\n",
    "    return Xtr, Str, Xts, Yts\n",
    "Xtr, Str, Xts, Yts = load_data('../input_data/mnist_dataset.npz', 'mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateBeta(S,prob,rho0,rho1):\n",
    "    S=S.astype(int)\n",
    "    rho=np.array([rho1,rho0])    \n",
    "    prob=prob[:,0]*(1-S[:])+prob[:,1]*(S[:])\n",
    "    beta=(prob[:]-rho[S].ravel())/(1-rho0-rho1)/prob[:]\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = np.load('../input_data/mnist_dataset.npz')\n",
    "dataset2 = np.load('../input_data/cifar_dataset.npz')\n",
    "size_image1 = 28\n",
    "dim_image1 = 1\n",
    "size_image2 = 32\n",
    "dim_image2 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.lib.npyio.NpzFile object at 0x10b7895f8>\n"
     ]
    }
   ],
   "source": [
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to store the data splits\n",
    "data_cache={}\n",
    "\n",
    "#transform dataset1\n",
    "Xtr1 = dataset1 ['Xtr'].astype(float)\n",
    "Str1 = dataset1 ['Str'].ravel()\n",
    "Xts1 = dataset1 ['Xts'].astype(float)\n",
    "Yts1 = dataset1 ['Yts'].ravel()\n",
    "scaler = StandardScaler()\n",
    "Xts1 = scaler.fit_transform(Xts1.T).T\n",
    "Xtr1 = scaler.fit_transform(Xtr1.T).T\n",
    "data_cache[1]=(Xtr1,Str1,Xts1,Yts1)\n",
    "\n",
    "#transform dataset2\n",
    "Xtr2 = dataset2 ['Xtr'].astype(float)\n",
    "Str2 = dataset2 ['Str'].ravel()\n",
    "Xts2 = dataset2 ['Xts'].astype(float)\n",
    "Yts2 = dataset2 ['Yts'].ravel()\n",
    "#scaler = StandardScaler()\n",
    "Xts2 = scaler.fit_transform(Xts2.T).T\n",
    "Xtr2 = scaler.fit_transform(Xtr2.T).T\n",
    "data_cache[2]=(Xtr2,Str2,Xts2,Yts2)\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(Xtr2)\n",
    "Xtr2=pca.transform(Xtr2)\n",
    "Xts2=pca.transform(Xts2)\n",
    "print('pca explained variance:',sum(pca.explained_variance_ratio_))\n",
    "if plot:\n",
    "    xplot=scaler.fit_transform(pca.inverse_transform(Xts2).T).T\n",
    "dset=1\n",
    "if plot:\n",
    "    plt.figure()\n",
    "    for i in range(0,30):\n",
    "        if dset==1:\n",
    "            image=xplot[i,].reshape(dim_image1,size_image1,size_image1).transpose([1, 2, 0])\n",
    "            plt.subplot(5, 6, i+1)\n",
    "            plt.imshow(image[:,:,:],interpolation='bicubic')\n",
    "            plt.title(Yts1[i])\n",
    "        else:\n",
    "            image=xplot[i,].reshape(dim_image2,size_image2,size_image2).transpose([1, 2, 0])\n",
    "            plt.subplot(5, 6, i+1)\n",
    "            plt.imshow(image[:,:,:],interpolation='bicubic')\n",
    "            plt.title(Yts2[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
